{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2548c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Image, display, display_png\n",
    "\n",
    "from mglyph_ml.dataset.glyph_dataset import GlyphDataset\n",
    "from mglyph_ml.dataset.manifest import DatasetManifest, ManifestSample\n",
    "from mglyph_ml.experiment.e1.experiment import ExperimentConfig, run_experiment\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    task_name=\"Experiment 1.2.1\",\n",
    "    task_tag=\"exp-1.2.1\",\n",
    "    dataset_path=\"data/universal.mglyph\",\n",
    "    gap_start_x=40.0,\n",
    "    gap_end_x=60.0,\n",
    "    quick=True,\n",
    "    seed=420,\n",
    "    max_iterations=5,\n",
    "    offline=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7ec157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = Path(\"../data/universal.mglyph\")\n",
    "\n",
    "\n",
    "def decode_png_bytes(png_bytes: bytes) -> np.ndarray:\n",
    "    buf = np.frombuffer(png_bytes, dtype=np.uint8)\n",
    "    img = cv2.imdecode(buf, cv2.IMREAD_COLOR)  # HWC, BGR, uint8\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_image_into_ndarray(archive: ZipFile, filename: str) -> np.ndarray:\n",
    "    png_bytes = archive.read(filename)\n",
    "    return decode_png_bytes(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f150f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading everything... this cell takes the longest time\n",
    "\n",
    "temp_archive = zipfile.ZipFile(path, \"r\")\n",
    "\n",
    "manifest_data = temp_archive.read(\"manifest.json\")\n",
    "manifest = DatasetManifest.model_validate_json(manifest_data)\n",
    "\n",
    "samples_all = manifest.samples[\"uni\"]\n",
    "\n",
    "# Create index mappings for each subset\n",
    "train_indices = [i for i, sample in enumerate(samples_all) if sample.x < 40.0 or sample.x >= 60]\n",
    "gap_indices = [i for i, sample in enumerate(samples_all) if sample.x >= 40.0 and sample.x < 60]\n",
    "test_indices = list(range(len(samples_all)))\n",
    "\n",
    "# Get sample subsets using indices\n",
    "manifest_samples_train: list[ManifestSample] = [samples_all[i] for i in train_indices]\n",
    "manifest_samples_gap: list[ManifestSample] = [samples_all[i] for i in gap_indices]\n",
    "manifest_samples_test: list[ManifestSample] = samples_all\n",
    "\n",
    "# Load all images once\n",
    "with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "    images_all = list(\n",
    "        executor.map(lambda sample: load_image_into_ndarray(temp_archive, sample.filename), samples_all)\n",
    "    )\n",
    "\n",
    "# Reference image subsets using indices\n",
    "images_train = [images_all[i] for i in train_indices]\n",
    "images_gap = [images_all[i] for i in gap_indices]\n",
    "images_test = images_all\n",
    "\n",
    "labels_train = [samples_all[i].x for i in train_indices]\n",
    "labels_gap = [samples_all[i].x for i in gap_indices]\n",
    "labels_test = [sample.x for sample in samples_all]\n",
    "\n",
    "temp_archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c546e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 12039\n",
      "First image type: <class 'numpy.ndarray'>\n",
      "First image dtype: uint8\n",
      "First image shape: (512, 512, 3)\n",
      "First image is ndarray: True\n",
      "First image in BGR format: True\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify BGR format and display sample image\n",
    "print(f\"Number of images: {len(images_train)}\")\n",
    "print(f\"First image type: {type(images_train[0])}\")\n",
    "print(f\"First image dtype: {images_train[0].dtype}\")\n",
    "print(f\"First image shape: {images_train[0].shape}\")\n",
    "print(f\"First image is ndarray: {isinstance(images_train[0], np.ndarray)}\")\n",
    "print(f\"First image in BGR format: {images_train[0].dtype == np.uint8 and len(images_train[0].shape) == 3}\")\n",
    "\n",
    "# Display one sample image (convert BGR to RGB for display)\n",
    "display_img = cv2.cvtColor(images_train[0], cv2.COLOR_BGR2RGB)\n",
    "display(Image(data=cv2.imencode('.png', display_img)[1].tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0898d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = GlyphDataset(images=images_train, labels=labels_train, transform=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglyph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

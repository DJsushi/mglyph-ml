{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2548c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import albumentations as A\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from clearml import Task\n",
    "from dotenv import load_dotenv\n",
    "from mglyph_ml.experiment.e1.util import load_image_into_ndarray\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from mglyph_ml.dataset.glyph_dataset import GlyphDataset\n",
    "from mglyph_ml.dataset.manifest import DatasetManifest, ManifestSample\n",
    "from mglyph_ml.experiment.e1.experiment import ExperimentConfig\n",
    "from mglyph_ml.experiment.e1.train_model import train_and_test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ec157",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../data/uni.mglyph\")\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    task_name=\"Experiment 1.2.1\",\n",
    "    task_tag=\"exp-1.2.1\",\n",
    "    dataset_path=\"../data/uni.mglyph\",\n",
    "    gap_start_x=40.0,\n",
    "    gap_end_x=60.0,\n",
    "    quick=True,\n",
    "    seed=420,\n",
    "    max_iterations=5,\n",
    "    offline=True,\n",
    ")\n",
    "\n",
    "Task.set_offline(config.offline)\n",
    "task: Task = Task.init(project_name=\"mglyph-ml\", task_name=config.task_name)\n",
    "task.add_tags(config.task_tag)\n",
    "task.connect(config)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading everything... this cell takes the longest time\n",
    "temp_archive = zipfile.ZipFile(path, \"r\")\n",
    "\n",
    "manifest_data = temp_archive.read(\"manifest.json\")\n",
    "manifest = DatasetManifest.model_validate_json(manifest_data)\n",
    "\n",
    "samples_all = manifest.samples[\"uni\"]\n",
    "\n",
    "# Create index mappings for each subset\n",
    "train_indices = [i for i, sample in enumerate(samples_all) if sample.x < 40.0 or sample.x >= 60]\n",
    "gap_indices = [i for i, sample in enumerate(samples_all) if sample.x >= 40.0 and sample.x < 60]\n",
    "test_indices = list(range(len(samples_all)))\n",
    "\n",
    "# Load all images once\n",
    "with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "    images_all = list(\n",
    "        executor.map(lambda sample: load_image_into_ndarray(temp_archive, sample.filename), samples_all)\n",
    "    )\n",
    "\n",
    "# Reference image subsets using indices\n",
    "images_train = [images_all[i] for i in train_indices]\n",
    "images_gap = [images_all[i] for i in gap_indices]\n",
    "images_test = images_all\n",
    "\n",
    "labels_train = [samples_all[i].x for i in train_indices]\n",
    "labels_gap = [samples_all[i].x for i in gap_indices]\n",
    "labels_test = [sample.x for sample in samples_all]\n",
    "\n",
    "temp_archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify RGB format and display sample image\n",
    "print(f\"Number of images: {len(images_train)}\")\n",
    "print(f\"First image type: {type(images_train[0])}\")\n",
    "print(f\"First image dtype: {images_train[0].dtype}\")\n",
    "print(f\"First image shape: {images_train[0].shape}\")\n",
    "print(f\"First image is ndarray: {isinstance(images_train[0], np.ndarray)}\")\n",
    "print(f\"First image in RGB format: {images_train[0].dtype == np.uint8 and len(images_train[0].shape) == 3}\")\n",
    "\n",
    "# Display one sample image (convert BGR to RGB for display)\n",
    "display(Image(data=cv2.imencode('.png', images_train[0])[1].tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "affine = A.Affine(\n",
    "    rotate=(-5, 5),\n",
    "    translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "    fit_output=False,\n",
    "    keep_ratio=True,\n",
    "    border_mode=cv2.BORDER_CONSTANT,\n",
    "    fill=255,\n",
    "    p=1.0,\n",
    ")\n",
    "normalize = A.Normalize(normalization=\"min_max\")\n",
    "to_tensor = ToTensorV2()\n",
    "pipeline = A.Compose([affine, normalize, to_tensor], seed=420)\n",
    "normalize_pipeline = A.Compose([normalize, to_tensor])\n",
    "\n",
    "def affine_and_normalize(image: np.ndarray) -> torch.Tensor:\n",
    "    return pipeline(image=image)[\"image\"]\n",
    "\n",
    "def just_normalize(image: np.ndarray) -> torch.Tensor:\n",
    "    return normalize_pipeline(image=image)[\"image\"]\n",
    "\n",
    "dataset_train = GlyphDataset(images=images_train, labels=labels_train, transform=affine_and_normalize)\n",
    "dataset_gap = GlyphDataset(images=images_gap, labels=labels_gap, transform=just_normalize)\n",
    "dataset_test = GlyphDataset(images=images_test, labels=labels_test, transform=just_normalize)\n",
    "\n",
    "image = dataset_train[2000][0].numpy().transpose(1, 2, 0) * 255.0\n",
    "print(image.shape)\n",
    "display(Image(data=cv2.imencode(\".png\", image)[1].tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = os.environ[\"MGML_DEVICE\"]\n",
    "\n",
    "train_and_test_model(\n",
    "    device=device,\n",
    "    dataset_train=dataset_train,\n",
    "    dataset_gap=dataset_gap,\n",
    "    dataset_test=dataset_test,\n",
    "    seed=420,\n",
    "    data_loader_num_workers=32,\n",
    "    batch_size=256,\n",
    "    quick=False,\n",
    "    max_epochs=20,\n",
    "    model_save_path=Path(\"../models/exp1.pt\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglyph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2548c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from clearml import Task\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from mglyph_ml.dataset.glyph_dataset import GlyphDataset\n",
    "from mglyph_ml.dataset.manifest import DatasetManifest, ManifestSample\n",
    "from mglyph_ml.experiment.e1.experiment import ExperimentConfig\n",
    "from mglyph_ml.experiment.e1.train_model import train_and_test_model\n",
    "from mglyph_ml.experiment.e1.util import load_image_into_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ec157",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../data/uni.mglyph\")\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    task_name=\"Experiment 1.2.1\",\n",
    "    task_tag=\"exp-1.2.1\",\n",
    "    dataset_path=Path(\"../data/uni.mglyph\"),\n",
    "    gap_start_x=40.0,\n",
    "    gap_end_x=60.0,\n",
    "    quick=True,\n",
    "    seed=420,\n",
    "    max_iterations=5,\n",
    "    offline=True,\n",
    ")\n",
    "\n",
    "Task.set_offline(config.offline)\n",
    "task: Task = Task.init(project_name=\"mglyph-ml\", task_name=config.task_name)\n",
    "task.add_tags(config.task_tag)\n",
    "task.connect(config)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading everything... this cell takes the longest time\n",
    "# Load the entire zip file into memory\n",
    "with open(path, \"rb\") as f:\n",
    "    temp_archive = ZipFile(BytesIO(f.read()))\n",
    "\n",
    "manifest_data = temp_archive.read(\"manifest.json\")\n",
    "manifest = DatasetManifest.model_validate_json(manifest_data)\n",
    "\n",
    "samples_0 = manifest.samples[\"0\"]  # this is where the training and validation data comes from\n",
    "samples_1 = manifest.samples[\"1\"]  # this is where the test data comes from\n",
    "\n",
    "# Create index mappings for each subset\n",
    "indices_train = [\n",
    "    i for i, sample in enumerate(samples_0) if sample.x < config.gap_start_x or sample.x >= config.gap_end_x\n",
    "]\n",
    "indices_gap = [\n",
    "    i for i, sample in enumerate(samples_0) if sample.x >= config.gap_start_x and sample.x < config.gap_end_x\n",
    "]\n",
    "indices_test = list(range(len(samples_1)))\n",
    "\n",
    "random.shuffle(indices_train)\n",
    "random.shuffle(indices_gap)\n",
    "random.shuffle(indices_test)\n",
    "\n",
    "indices_train = indices_train[: len(indices_train) // 2]\n",
    "indices_gap = indices_gap[: len(indices_gap) // 2]\n",
    "indices_test = indices_test[: len(indices_test) // 2]\n",
    "\n",
    "affine = A.Affine(\n",
    "    rotate=(-5, 5),\n",
    "    translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "    fit_output=False,\n",
    "    keep_ratio=True,\n",
    "    border_mode=cv2.BORDER_CONSTANT,\n",
    "    fill=255,\n",
    "    p=1.0,\n",
    ")\n",
    "normalize = A.Normalize(normalization=\"min_max\")\n",
    "to_tensor = ToTensorV2()\n",
    "pipeline = A.Compose([affine, normalize, to_tensor], seed=420)\n",
    "normalize_pipeline = A.Compose([normalize, to_tensor])\n",
    "\n",
    "\n",
    "def affine_and_normalize(image: np.ndarray) -> torch.Tensor:\n",
    "    return pipeline(image=image)[\"image\"]\n",
    "\n",
    "\n",
    "def just_normalize(image: np.ndarray) -> torch.Tensor:\n",
    "    return normalize_pipeline(image=image)[\"image\"]\n",
    "\n",
    "\n",
    "print(\"loading images\")\n",
    "\n",
    "# Load all images from memory (much faster than disk access)\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    images_train = list(\n",
    "        executor.map(lambda i: affine_and_normalize(load_image_into_ndarray(temp_archive, samples_0[i].filename)), indices_train)\n",
    "    )\n",
    "    # images_0 = list(\n",
    "    #     executor.map(lambda sample: load_image_into_ndarray(temp_archive, sample.filename), samples_0)\n",
    "    # )\n",
    "    # images_1 = list(\n",
    "    #     executor.map(lambda sample: load_image_into_ndarray(temp_archive, sample.filename), samples_1)\n",
    "    # )\n",
    "\n",
    "# Reference image subsets using indices\n",
    "# images_train = [images_0[i] for i in indices_train]\n",
    "# images_gap = [images_0[i] for i in indices_gap]\n",
    "# images_test = [images_1[i] for i in indices_test]\n",
    "\n",
    "# labels_train = [samples_0[i].x for i in indices_train]\n",
    "# labels_gap = [samples_0[i].x for i in indices_gap]\n",
    "# labels_test = [samples_1[i].x for i in indices_test]\n",
    "\n",
    "temp_archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify RGB format and display sample image\n",
    "print(f\"Number of images: {len(images_train)}\")\n",
    "print(f\"First image type: {type(images_train[0])}\")\n",
    "print(f\"First image dtype: {images_train[0].dtype}\")\n",
    "print(f\"First image shape: {images_train[0].shape}\")\n",
    "print(f\"Training image count: {len(images_train)}\")\n",
    "\n",
    "# Display one sample image (convert BGR to RGB for display)\n",
    "display(Image(data=cv2.imencode('.png', images_train[21].numpy().transpose(1, 2, 0) * 255.0)[1].tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = GlyphDataset(images=images_train, labels=labels_train)\n",
    "dataset_gap = GlyphDataset(images=images_gap, labels=labels_gap)\n",
    "dataset_test = GlyphDataset(images=images_test, labels=labels_test)\n",
    "\n",
    "image = dataset_train[2000][0].numpy().transpose(1, 2, 0) * 255.0\n",
    "print(image.shape)\n",
    "display(Image(data=cv2.imencode(\".png\", image)[1].tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = os.environ[\"MGML_DEVICE\"]\n",
    "\n",
    "train_and_test_model(\n",
    "    device=device,\n",
    "    dataset_train=dataset_train,\n",
    "    dataset_gap=dataset_gap,\n",
    "    dataset_test=dataset_test,\n",
    "    seed=420,\n",
    "    data_loader_num_workers=32,\n",
    "    batch_size=256,\n",
    "    quick=False,\n",
    "    max_epochs=20,\n",
    "    model_save_path=Path(\"../models/exp1.pt\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglyph-ml (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

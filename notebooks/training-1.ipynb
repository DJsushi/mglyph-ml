{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:14:55.204320Z",
     "start_time": "2025-11-06T15:14:55.201200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import mglyph as mg\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import ImageReadMode\n",
    "\n",
    "import mglyph_ml.lib as lib\n",
    "from mglyph_ml.image_provider import GlyphProvider\n",
    "from mglyph_ml.manifest_parsing import Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1915fa4bf5184",
   "metadata": {},
   "source": [
    "# Building the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8918c4a38c979919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:01.789542Z",
     "start_time": "2025-11-06T15:14:55.252450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170ec23cea3a479fa10db86ebf7ae957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Exporting square 1.0.0:', max=1000, style=ProgressStyle(bar_color='cornflowe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/PIL/ImageFile.py:648\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, tile, bufsize)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     fh = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m()\n\u001b[32m    649\u001b[39m     fp.flush()\n",
      "\u001b[31mAttributeError\u001b[39m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m     canvas.tr.scale(mg.lerp(x, \u001b[32m0.2\u001b[39m, \u001b[32m0.95\u001b[39m))\n\u001b[32m      3\u001b[39m     canvas.rect(canvas.top_left, canvas.bottom_right, color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport_glyph\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msquare\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglyph_set\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/src/mglyph_ml/lib.py:7\u001b[39m, in \u001b[36mexport_glyph\u001b[39m\u001b[34m(drawer, name, glyph_set)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexport_glyph\u001b[39m(drawer: Callable[[\u001b[38;5;28mfloat\u001b[39m, mg.Canvas], \u001b[38;5;28;01mNone\u001b[39;00m], name: \u001b[38;5;28mstr\u001b[39m, glyph_set: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mmg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrawer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshort_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/glyphs-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mglyph_set\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.mglyph\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/mglyph/mglyph.py:617\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(drawer, name, short_name, author, email, version, author_public, creation_time, path, canvas_parameters, resolution, xvalues, silent)\u001b[39m\n\u001b[32m    615\u001b[39m image = render(drawer, resolution, x, canvas_parameters, compress=\u001b[33m'\u001b[39m\u001b[33mpil\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    616\u001b[39m data = BytesIO()\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpil\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPNG\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress_level\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m data.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    619\u001b[39m zf.writestr(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_digits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m, data.read())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/PIL/Image.py:2571\u001b[39m, in \u001b[36mImage.save\u001b[39m\u001b[34m(self, fp, format, **params)\u001b[39m\n\u001b[32m   2568\u001b[39m     fp = cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2571\u001b[39m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   2573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/PIL/PngImagePlugin.py:1497\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, filename, chunk, save_all)\u001b[39m\n\u001b[32m   1493\u001b[39m     single_im = _write_multiple_frames(\n\u001b[32m   1494\u001b[39m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[32m   1495\u001b[39m     )\n\u001b[32m   1496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m     \u001b[43mImageFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Tile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[32m   1504\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info.chunks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/PIL/ImageFile.py:652\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, tile, bufsize)\u001b[39m\n\u001b[32m    650\u001b[39m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io.UnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[33m\"\u001b[39m\u001b[33mflush\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    654\u001b[39m     fp.flush()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/PIL/ImageFile.py:678\u001b[39m, in \u001b[36m_encode_tile\u001b[39m\u001b[34m(im, fp, tile, bufsize, fh, exc)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m         errcode, data = \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m:]\n\u001b[32m    679\u001b[39m         fp.write(data)\n\u001b[32m    680\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def square(x: float, canvas: mg.Canvas):\n",
    "    canvas.tr.scale(mg.lerp(x, 0.2, 0.95))\n",
    "    canvas.rect(canvas.top_left, canvas.bottom_right, color=\"red\")\n",
    "\n",
    "\n",
    "lib.export_glyph(square, name=\"square\", glyph_set=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1e3af894b4228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.444361Z",
     "start_time": "2025-11-06T15:15:01.842458Z"
    }
   },
   "outputs": [],
   "source": [
    "# here, we need to load the glyph and convert it into a format that'll be accepted by the NN\n",
    "glyph_location: str = \"data/glyphs-1/square.mglyph\"\n",
    "glyph_provider: GlyphProvider = GlyphProvider(glyph_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151028e6734bfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.667984Z",
     "start_time": "2025-11-06T15:15:04.655905Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Creating the PyTorch Dataset and DataLoader\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, zip_path: str, min_label: float | None = None, max_label: float | None = None, augment: bool = False, normalize: bool = True):\n",
    "        self.zip_path = zip_path\n",
    "        # Do not use global archive; always open as needed\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
    "            manifest = zf.read(\"metadata.json\")\n",
    "        self.manifest = Manifest.model_validate_json(manifest)\n",
    "        # Use raw labels (0-100) for splitting\n",
    "        samples = [(sample.filename, sample.x) for sample in self.manifest.images]\n",
    "        if min_label is not None:\n",
    "            samples = [s for s in samples if s[1] >= min_label]\n",
    "        if max_label is not None:\n",
    "            samples = [s for s in samples if s[1] < max_label]\n",
    "        self.samples = samples\n",
    "        self.archive = None\n",
    "        self.augment = augment\n",
    "        self.normalize = normalize\n",
    "        # Normalization parameters (ImageNet defaults) - applied after converting to [0,1] floats\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "        if augment:\n",
    "            # Use small translate_percent (±5%) and constant white padding to avoid black fill from translations/rotations\n",
    "            # Use OpenCV border_mode and 'value' for fill color (uint8).\n",
    "            self.transform = A.Compose([\n",
    "                A.Affine(rotate=(-5, 5),\n",
    "                         translate_percent=(-0.20, 0.20),\n",
    "                         fit_output=False,\n",
    "                         keep_ratio=True,\n",
    "                         border_mode=cv2.BORDER_CONSTANT,\n",
    "                         fill=255\n",
    "                         ),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def _ensure_archive(self):\n",
    "        if self.archive is None:\n",
    "            self.archive = zipfile.ZipFile(self.zip_path, 'r')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, float]:\n",
    "        self._ensure_archive()\n",
    "        filename, label = self.samples[index]\n",
    "        img_bytes = self.archive.read(filename)\n",
    "        # Use PIL to decode reliably and get an HWC uint8 numpy array\n",
    "        img_pil = PILImage.open(BytesIO(img_bytes))\n",
    "        # If image has alpha channel, paste onto white background\n",
    "        if img_pil.mode in ('RGBA', 'LA') or (img_pil.mode == 'P' and 'transparency' in img_pil.info):\n",
    "            background = PILImage.new('RGBA', img_pil.size, (255, 255, 255, 255))\n",
    "            background.paste(img_pil, mask=img_pil.split()[-1])\n",
    "            img_pil = background.convert('RGB')\n",
    "        else:\n",
    "            img_pil = img_pil.convert('RGB')\n",
    "        img_np = np.array(img_pil)  # H x W x C, dtype=uint8, values 0-255\n",
    "        # Apply augmentation on HWC uint8 image\n",
    "        if self.augment and self.transform is not None:\n",
    "            augmented = self.transform(image=img_np)\n",
    "            img_np = augmented['image']\n",
    "        # Convert to CHW float tensor in [0,1] for the model\n",
    "        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0\n",
    "        # Optionally normalize to zero-mean/unit-std using mean/std (channels first)\n",
    "        if self.normalize:\n",
    "            img_tensor = (img_tensor - self.mean[:, None, None]) / self.std[:, None, None]\n",
    "        label = label / 100.0\n",
    "        return img_tensor, label\n",
    "\n",
    "# Configure DataLoader workers and create datasets/loaders\n",
    "recommended_workers = min(4, os.cpu_count() or 1)\n",
    "train_set_1 = ImageDataset(zip_path=glyph_location, max_label=80, augment=True, normalize=True)\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_set_1, shuffle=True, batch_size=8, num_workers=recommended_workers, pin_memory=True)\n",
    "test_set_1 = ImageDataset(zip_path=glyph_location, min_label=80, augment=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d25959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helpers: un-normalize tensors and visualize a batch from a DataLoader\n",
    "import math\n",
    "\n",
    "def unnormalize_tensor_to_uint8(img_tensor: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert a CHW torch tensor that was normalized (with mean/std) back to HWC uint8 for display.\n",
    "    img_tensor: (C, H, W) in normalized float format\n",
    "    returns H x W x C uint8 numpy array\"\"\"\n",
    "    # move to cpu and clone to avoid modifying original\n",
    "    img = img_tensor.detach().cpu().clone()\n",
    "    if img.ndim == 4:\n",
    "        # take first element if batch provided\n",
    "        img = img[0]\n",
    "    # unnormalize: img = img * std + mean\n",
    "    img = img * std[:, None, None] + mean[:, None, None]\n",
    "    img = torch.clamp(img, 0.0, 1.0)\n",
    "    img_np = (img.permute(1, 2, 0).numpy() * 255.0).round().astype(np.uint8)\n",
    "    return img_np\n",
    "\n",
    "def show_batch_from_loader(loader: DataLoader, n: int = 9, mean: torch.Tensor = None, std: torch.Tensor = None):\n",
    "    \"\"\"Grab one batch from loader and show up to n images (handles normalized tensors).\"\"\"\n",
    "    batch = next(iter(loader))\n",
    "    imgs, labels = batch\n",
    "    # default mean/std (should match dataset)\n",
    "    if mean is None or std is None:\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "    count = min(n, imgs.shape[0])\n",
    "    cols = int(math.sqrt(n))\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    for i in range(count):\n",
    "        img_vis = unnormalize_tensor_to_uint8(imgs[i], mean, std)\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.imshow(img_vis)\n",
    "        ax.set_title(f\"{labels[i].item() * 100:.1f}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demo: show a batch from the training loader (if available)\n",
    "try:\n",
    "    show_batch_from_loader(train_loader_1, n=9, mean=train_set_1.mean, std=train_set_1.std)\n",
    "except Exception as e:\n",
    "    print('Unable to show batch automatically:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a31be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: idx=235, shape=(3, 512, 512), dtype=torch.float32, min=-2.035714, max=2.640000, label=0.235\n",
      "sample 1: idx=717, shape=(3, 512, 512), dtype=torch.float32, min=-2.035714, max=2.640000, label=0.717\n",
      "sample 2: idx=62, shape=(3, 512, 512), dtype=torch.float32, min=-2.035714, max=2.640000, label=0.062\n",
      "train samples: 800, recommended num_workers=4\n"
     ]
    }
   ],
   "source": [
    "# Quick dataset sanity checks: show shapes and value ranges for a few samples\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(train_set_1) - 1)\n",
    "    img, label = train_set_1[idx]\n",
    "    print(f\"sample {i}: idx={idx}, shape={tuple(img.shape)}, dtype={img.dtype}, min={img.min().item():.6f}, max={img.max().item():.6f}, label={label:.3f}\")\n",
    "\n",
    "# Also show counts and recommended DataLoader workers setting\n",
    "total = len(train_set_1)\n",
    "recommended_workers = min(4, os.cpu_count() or 1)\n",
    "print(f\"train samples: {total}, recommended num_workers={recommended_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3d8586b9ced63",
   "metadata": {},
   "source": [
    "Now that we've imported the zip file and parsed the manifest, we can build our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6e885648b76d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.709721Z",
     "start_time": "2025-11-06T15:15:04.705795Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlyphRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input: (batch, 3, 512, 512)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)  # (3,512,512) -> (16,512,512)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # (16,512,512) -> (16,256,256)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  # (16,256,256) -> (32,256,256)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # (32,256,256) -> (32,128,128)\n",
    "        # Reduce spatial size before FC to lower memory: adaptive pool to 8x8\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((8, 8))  # (32,128,128) -> (32,8,8)\n",
    "        # Flatten: 32*8*8 = 2048 (much smaller than 32*128*128)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.adaptivepool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eafb9ca5adfca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.758313Z",
     "start_time": "2025-11-06T15:15:04.754392Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_data_loader: DataLoader[ImageDataset]) -> GlyphRegressor:\n",
    "    model = GlyphRegressor()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_data_loader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:\n",
    "                avg_loss = running_loss / 20\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {avg_loss:.3f}')\n",
    "                running_loss = 0.0  # Reset after printing\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b43806366826e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T16:27:25.219312Z",
     "start_time": "2025-11-06T16:27:16.663989Z"
    }
   },
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "Caught BadZipFile in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_223425/3261872834.py\", line 58, in __getitem__\n    img_bytes = self.archive.read(filename)\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1602, in read\n    with self.open(name, \"r\", pwd) as fp:\n         ~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1660, in open\n    raise BadZipFile(\"Bad magic number for file header\")\nzipfile.BadZipFile: Bad magic number for file header\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadZipFile\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After training, evaluate 10 random samples from the test set:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Generate 10 random indices from the test set\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_data_loader)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m     running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mBadZipFile\u001b[39m: Caught BadZipFile in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_223425/3261872834.py\", line 58, in __getitem__\n    img_bytes = self.archive.read(filename)\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1602, in read\n    with self.open(name, \"r\", pwd) as fp:\n         ~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1660, in open\n    raise BadZipFile(\"Bad magic number for file header\")\nzipfile.BadZipFile: Bad magic number for file header\n"
     ]
    }
   ],
   "source": [
    "# After training, evaluate 10 random samples from the test set:\n",
    "model = train(train_loader_1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate 10 random indices from the test set\n",
    "    random_indices = torch.randint(0, len(test_set_1), size=(10,)).tolist()\n",
    "    for idx in random_indices:\n",
    "        img, true_label = test_set_1[idx]\n",
    "        # add batch dimension: (C,H,W) -> (1,C,H,W)\n",
    "        img_batch = img.unsqueeze(0)\n",
    "        pred = model(img_batch).item()\n",
    "        print(f\"True: {true_label * 100:.1f}, Predicted: {pred * 100:.1f}\")\n",
    "    print(\"=====train set======\")\n",
    "    random_indices = torch.randint(0, len(train_set_1), size=(10,)).tolist()\n",
    "    for idx in random_indices:\n",
    "        img, true_label = train_set_1[idx]\n",
    "        # add batch dimension: (C,H,W) -> (1,C,H,W)\n",
    "        img_batch = img.unsqueeze(0)\n",
    "        pred = model(img_batch).item()\n",
    "        print(f\"True: {true_label * 100:.1f}, Predicted: {pred * 100:.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglyph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:14:55.204320Z",
     "start_time": "2025-11-06T15:14:55.201200Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import mglyph as mg\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import ImageReadMode\n",
    "import albumentations as A\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from mglyph_ml.manifest_parsing import Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1915fa4bf5184",
   "metadata": {},
   "source": [
    "# Building the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8918c4a38c979919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:01.789542Z",
     "start_time": "2025-11-06T15:14:55.252450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ba788505764d5c9d14d28cfa8a787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Exporting square 1.0.0:', max=1000, style=ProgressStyle(bar_color='cornflowe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting square 1.0.0 finished!\n"
     ]
    }
   ],
   "source": [
    "def square(x: float, canvas: mg.Canvas):\n",
    "    canvas.tr.scale(mg.lerp(x, 0.2, 0.95))\n",
    "    canvas.rect(canvas.top_left, canvas.bottom_right, color=\"red\")\n",
    "\n",
    "\n",
    "lib.export_glyph(square, name=\"square\", glyph_set=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f1e3af894b4228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.444361Z",
     "start_time": "2025-11-06T15:15:01.842458Z"
    }
   },
   "outputs": [],
   "source": [
    "# here, we need to load the glyph and convert it into a format that'll be accepted by the NN\n",
    "glyph_location: str = \"data/glyphs-1/square.mglyph\"\n",
    "\n",
    "# it's a ZIP file, so we need to unzip it into memory\n",
    "archive: ZipFile = zipfile.ZipFile(glyph_location)\n",
    "\n",
    "# We import the manifest as a Pydantic model, so it's much easier to work with later\n",
    "manifest = archive.read(\"metadata.json\")\n",
    "manifest = Manifest.model_validate_json(manifest)\n",
    "\n",
    "image_size = 512  # Use original size, no downscaling\n",
    "images: list[tuple[Image.Image, float]] = []\n",
    "for image in manifest.images:\n",
    "    # Load as RGB, do not resize\n",
    "    image_data = Image.open(BytesIO(archive.read(image.filename))).convert('RGB')\n",
    "    images.append((image_data, image.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4151028e6734bfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.667984Z",
     "start_time": "2025-11-06T15:15:04.655905Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Creating the PyTorch Dataset and DataLoader\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, zip_path: str, min_label: float | None = None, max_label: float | None = None, augment: bool = False, normalize: bool = True):\n",
    "        self.zip_path = zip_path\n",
    "        # Do not use global archive; always open as needed\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
    "            manifest = zf.read(\"metadata.json\")\n",
    "        self.manifest = Manifest.model_validate_json(manifest)\n",
    "        # Use raw labels (0-100) for splitting\n",
    "        samples = [(sample.filename, sample.x) for sample in self.manifest.images]\n",
    "        if min_label is not None:\n",
    "            samples = [s for s in samples if s[1] >= min_label]\n",
    "        if max_label is not None:\n",
    "            samples = [s for s in samples if s[1] < max_label]\n",
    "        self.samples = samples\n",
    "        self.archive = None\n",
    "        self.augment = augment\n",
    "        self.normalize = normalize\n",
    "        # Normalization parameters (ImageNet defaults) - applied after converting to [0,1] floats\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "        if augment:\n",
    "            # Use small translate_percent (±5%) and constant white padding to avoid black fill from translations/rotations\n",
    "            # Use OpenCV border_mode and 'value' for fill color (uint8).\n",
    "            self.transform = A.Compose([\n",
    "                A.Affine(rotate=(-5, 5),\n",
    "                         translate_percent=(-0.20, 0.20),\n",
    "                         fit_output=False,\n",
    "                         keep_ratio=True,\n",
    "                         border_mode=cv2.BORDER_CONSTANT,\n",
    "                         fill=255\n",
    "                         ),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def _ensure_archive(self):\n",
    "        if self.archive is None:\n",
    "            self.archive = zipfile.ZipFile(self.zip_path, 'r')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, float]:\n",
    "        self._ensure_archive()\n",
    "        filename, label = self.samples[index]\n",
    "        img_bytes = self.archive.read(filename)\n",
    "        # Use PIL to decode reliably and get an HWC uint8 numpy array\n",
    "        img_pil = PILImage.open(BytesIO(img_bytes))\n",
    "        # If image has alpha channel, paste onto white background\n",
    "        if img_pil.mode in ('RGBA', 'LA') or (img_pil.mode == 'P' and 'transparency' in img_pil.info):\n",
    "            background = PILImage.new('RGBA', img_pil.size, (255, 255, 255, 255))\n",
    "            background.paste(img_pil, mask=img_pil.split()[-1])\n",
    "            img_pil = background.convert('RGB')\n",
    "        else:\n",
    "            img_pil = img_pil.convert('RGB')\n",
    "        img_np = np.array(img_pil)  # H x W x C, dtype=uint8, values 0-255\n",
    "        # Apply augmentation on HWC uint8 image\n",
    "        if self.augment and self.transform is not None:\n",
    "            augmented = self.transform(image=img_np)\n",
    "            img_np = augmented['image']\n",
    "        # Convert to CHW float tensor in [0,1] for the model\n",
    "        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0\n",
    "        # Optionally normalize to zero-mean/unit-std using mean/std (channels first)\n",
    "        if self.normalize:\n",
    "            img_tensor = (img_tensor - self.mean[:, None, None]) / self.std[:, None, None]\n",
    "        label = label / 100.0\n",
    "        return img_tensor, label\n",
    "\n",
    "# Configure DataLoader workers and create datasets/loaders\n",
    "recommended_workers = min(4, os.cpu_count() or 1)\n",
    "train_set_1 = ImageDataset(zip_path=glyph_location, max_label=80, augment=True, normalize=True)\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_set_1, shuffle=True, batch_size=8, num_workers=recommended_workers, pin_memory=True)\n",
    "test_set_1 = ImageDataset(zip_path=glyph_location, min_label=80, augment=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d25959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helpers: un-normalize tensors and visualize a batch from a DataLoader\n",
    "import math\n",
    "\n",
    "def unnormalize_tensor_to_uint8(img_tensor: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert a CHW torch tensor that was normalized (with mean/std) back to HWC uint8 for display.\n",
    "    img_tensor: (C, H, W) in normalized float format\n",
    "    returns H x W x C uint8 numpy array\"\"\"\n",
    "    # move to cpu and clone to avoid modifying original\n",
    "    img = img_tensor.detach().cpu().clone()\n",
    "    if img.ndim == 4:\n",
    "        # take first element if batch provided\n",
    "        img = img[0]\n",
    "    # unnormalize: img = img * std + mean\n",
    "    img = img * std[:, None, None] + mean[:, None, None]\n",
    "    img = torch.clamp(img, 0.0, 1.0)\n",
    "    img_np = (img.permute(1, 2, 0).numpy() * 255.0).round().astype(np.uint8)\n",
    "    return img_np\n",
    "\n",
    "def show_batch_from_loader(loader: DataLoader, n: int = 9, mean: torch.Tensor = None, std: torch.Tensor = None):\n",
    "    \"\"\"Grab one batch from loader and show up to n images (handles normalized tensors).\"\"\"\n",
    "    batch = next(iter(loader))\n",
    "    imgs, labels = batch\n",
    "    # default mean/std (should match dataset)\n",
    "    if mean is None or std is None:\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "    count = min(n, imgs.shape[0])\n",
    "    cols = int(math.sqrt(n))\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    for i in range(count):\n",
    "        img_vis = unnormalize_tensor_to_uint8(imgs[i], mean, std)\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.imshow(img_vis)\n",
    "        ax.set_title(f\"{labels[i].item() * 100:.1f}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demo: show a batch from the training loader (if available)\n",
    "try:\n",
    "    show_batch_from_loader(train_loader_1, n=9, mean=train_set_1.mean, std=train_set_1.std)\n",
    "except Exception as e:\n",
    "    print('Unable to show batch automatically:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a31be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: idx=256, shape=(3, 512, 512), dtype=torch.float32, min=-2.035714, max=2.640000, label=0.256\n",
      "sample 1: idx=281, shape=(3, 512, 512), dtype=torch.float32, min=-2.035714, max=2.640000, label=0.281\n",
      "sample 2: idx=353, shape=(3, 512, 512), dtype=torch.float32, min=-2.035714, max=2.640000, label=0.353\n",
      "train samples: 800, recommended num_workers=4\n"
     ]
    }
   ],
   "source": [
    "# Quick dataset sanity checks: show shapes and value ranges for a few samples\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(train_set_1) - 1)\n",
    "    img, label = train_set_1[idx]\n",
    "    print(f\"sample {i}: idx={idx}, shape={tuple(img.shape)}, dtype={img.dtype}, min={img.min().item():.6f}, max={img.max().item():.6f}, label={label:.3f}\")\n",
    "\n",
    "# Also show counts and recommended DataLoader workers setting\n",
    "total = len(train_set_1)\n",
    "recommended_workers = min(4, os.cpu_count() or 1)\n",
    "print(f\"train samples: {total}, recommended num_workers={recommended_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3d8586b9ced63",
   "metadata": {},
   "source": [
    "Now that we've imported the zip file and parsed the manifest, we can build our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d6e885648b76d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.709721Z",
     "start_time": "2025-11-06T15:15:04.705795Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlyphRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input: (batch, 3, 512, 512)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)  # (3,512,512) -> (16,512,512)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # (16,512,512) -> (16,256,256)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  # (16,256,256) -> (32,256,256)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # (32,256,256) -> (32,128,128)\n",
    "        # Reduce spatial size before FC to lower memory: adaptive pool to 8x8\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((8, 8))  # (32,128,128) -> (32,8,8)\n",
    "        # Flatten: 32*8*8 = 2048 (much smaller than 32*128*128)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.adaptivepool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530eafb9ca5adfca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T15:15:04.758313Z",
     "start_time": "2025-11-06T15:15:04.754392Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_data_loader: DataLoader[ImageDataset]) -> GlyphRegressor:\n",
    "    model = GlyphRegressor()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_data_loader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:\n",
    "                avg_loss = running_loss / 20\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {avg_loss:.3f}')\n",
    "                running_loss = 0.0  # Reset after printing\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c5b43806366826e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T16:27:25.219312Z",
     "start_time": "2025-11-06T16:27:16.663989Z"
    }
   },
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "Caught BadZipFile in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_205167/3261872834.py\", line 58, in __getitem__\n    img_bytes = self.archive.read(filename)\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1602, in read\n    with self.open(name, \"r\", pwd) as fp:\n         ~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1660, in open\n    raise BadZipFile(\"Bad magic number for file header\")\nzipfile.BadZipFile: Bad magic number for file header\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadZipFile\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After training, evaluate 10 random samples from the test set:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Generate 10 random indices from the test set\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_data_loader)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m     running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mBadZipFile\u001b[39m: Caught BadZipFile in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/djsushi/School/BAK/mglyph-ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_205167/3261872834.py\", line 58, in __getitem__\n    img_bytes = self.archive.read(filename)\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1602, in read\n    with self.open(name, \"r\", pwd) as fp:\n         ~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/zipfile/__init__.py\", line 1660, in open\n    raise BadZipFile(\"Bad magic number for file header\")\nzipfile.BadZipFile: Bad magic number for file header\n"
     ]
    }
   ],
   "source": [
    "# After training, evaluate 10 random samples from the test set:\n",
    "model = train(train_loader_1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate 10 random indices from the test set\n",
    "    random_indices = torch.randint(0, len(test_set_1), size=(10,)).tolist()\n",
    "    for idx in random_indices:\n",
    "        img, true_label = test_set_1[idx]\n",
    "        # add batch dimension: (C,H,W) -> (1,C,H,W)\n",
    "        img_batch = img.unsqueeze(0)\n",
    "        pred = model(img_batch).item()\n",
    "        print(f\"True: {true_label * 100:.1f}, Predicted: {pred * 100:.1f}\")\n",
    "    print(\"=====train set======\")\n",
    "    random_indices = torch.randint(0, len(train_set_1), size=(10,)).tolist()\n",
    "    for idx in random_indices:\n",
    "        img, true_label = train_set_1[idx]\n",
    "        # add batch dimension: (C,H,W) -> (1,C,H,W)\n",
    "        img_batch = img.unsqueeze(0)\n",
    "        pred = model(img_batch).item()\n",
    "        print(f\"True: {true_label * 100:.1f}, Predicted: {pred * 100:.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglyph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
